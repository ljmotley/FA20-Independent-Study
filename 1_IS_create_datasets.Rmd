---
title: "Create Datasets"
author: Luke Motley
output: html_notebook
editor_options: 
  chunk_output_type: inline
---
| **PURPOSE** 
| To create the datasets used in my independent study project, which investigates the relationship between changes in engagement with online resources and the ability to work from home at the onset of the COVID-19 pandemic. The final product is a dataset at the county-level with weekly observations for Google Trends search intensity metrics (related to online learning resources) and Zearn math data engagement and achievement measures. It contains dummy variables indicating the weeks since COVID. The latter is the set of interest for my original analyses, but I replicate the findings by Bacher-Hicks et al. (2020) using the full Google Trends data. The Google Trends set has observations for June 2016 - June 2020, whereas the Zearn (and combined) set has observations for Jan 2019 - Present.
| 
| **DATA SOURCES**
| *Google Trends Data* generously supplied by Professor Andrew Bacher-Hicks, Professor Joshua Goodman, and Associate Policy Researcher Christine Mulhern. Their paper has been conditionally accepted by the *Journal of Public Economics*. (The paper: https://www.nber.org/system/files/working_papers/w27555/w27555.pdf).
| 
| *Zearn Data* taken from the Opportunity Insights Economic Tracker Data. (Available at: https://opportunityinsights.org/wp-content/uploads/2020/05/tracker_paper.pdf).
| 
| *Employment data and Teleworkability scores* from the paper "How Many Jobs Can be Done at Home?" by Professor Jonathan Dingel and Professor Brent Neiman, who built their sets using data from the U.S. Bureau of Labor Statistics and O*NET Resource Center. (Available at: https://github.com/jdingel/DingelNeiman-workathome).
| 
| *Population and median household income data* from the Census Bureau's American Community Survey in 2018. (Available at: https://www.census.gov/programs-surveys/acs/data.html).
|
| *DMA crosswalk* from the Harvard Dataverse, which uses data directly from Nielsen. (Available at: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/IVXEHT).
|
| *CBSA crosswalk* from the National Bureau of Economic Research (Available at: https://www.nber.org/research/data/census-core-based-statistical-area-cbsa-federal-information-processing-series-fips-county-crosswalk)
|
| **REFERENCES**
| Bacher-Hicks, Andrew, Joshua Goodman, and Christine Mulhern. 2020. "Inequality in Household Adaptation to Schooling Shocks: Covid-Induced
|     Online Learning Engagement in Real Time," *NBER Working Papers 27555, National Bureau of Economic Research, Inc.*
| 
| Chetty, Raj, John Friedman, Nathaniel Hendren, Michael Stepner, and the Opportunity Insights Team. 2020. "The Economic Impacts of COVID-19: 
|     Evidence from a New Public Database Built Using Private Sector Data."
| 
| Dingel, Jonathan and Brent Neiman. 2020. "How many jobs can be done at home?" *Journal of Public Economics, vol 189*.
|
| **CODE**
```{r}
# Author: Luke Motley
# Purpose: Create the datasets used in my independent study project.
setwd("~/Desktop/IS")
library(reshape2)
library(dplyr)
library(tidyr)
library(readxl)
library(writexl)

na_zero <- function(df) {
  df[is.na(df)] <- 0
  return(df)
}
```
**Prepare Google Data Set**
```{r}
gtrends_dma <- read_excel("trends_dma_data.xlsx", sheet = "Sheet1")
dmarank_to_dmacode <- read_excel("trends_dma_data.xlsx", sheet = "Sheet2")
dma_to_county <- read_excel("county_dma.xlsx") 

# must convert state and county FIPS to full FIPS code before joining
gtrends_county <- dma_to_county %>% 
  mutate(FIPS = as.numeric(paste(STATEFP, formatC(CNTYFP, width=3, flag="0"), sep=""))) %>% 
  left_join(dmarank_to_dmacode, by = "DMARANK") %>% 
  select(COUNTY, STATE, FIPS, DMA.x, DMAINDEX) %>% 
  rename(DMA = DMA.x) %>% 
  right_join(gtrends_dma, by = "DMAINDEX") %>% 
  select(COUNTY, FIPS, dma_name, week, specific1:placebo2)
```
| **Prepare Zearn Data Set**
| *Note: here we want NAs to be introduced, as they allow us to coalesce and then signify if the observation was a break observation. Hence, we can easily replace the NAs with 0 to create our break indicator .*
```{r}
zearn_county <- read_excel("Zearn_County_Weekly_4.xlsx")

# convert measurement variables to numric
zearn_county[5:8] <- sapply(zearn_county[5:8], as.numeric)
# adjust date format to match Google data ("week" is day the week began), combine  break and normal variables, create indicator for break
zearn_county <- zearn_county %>% 
  mutate(week = as.Date(as.character(gsub(" ", "", paste(year,"/", month,"/", day_endofweek), fixed = TRUE), format="%Y/%m/%d")) - 7) %>% 
  rename(FIPS = countyfips) %>%
  mutate(engagement = coalesce(engagement, break_engagement), badges = coalesce(badges, break_badges)) %>% 
  mutate("break" = na_zero(break_engagement / break_engagement)) %>% 
  select(FIPS, week, engagement, badges, "break")
```
| **Prepare Employment Datasets**
| *Note: We see a few trivial warnings. There is one missing household income observation and we again want NA's to be introduced so we can make them 0's.*
```{r}
emp_data <- read_excel("Occupational Data.xlsx")
hh_inc_county <- read_excel("ACS_med_hh_inc_2018.xlsx")
pop_county <- read_excel("ACS_2018_population_county.xlsx")
tele_data <- read_excel("teleworkable.xlsx")
cbsa_to_county <- read_excel("cbsa_crosswalk_2.xlsx")

tele_data <- tele_data %>% rename(OCC_TITLE = OES_TITLE)
# again get full FIPS code
cbsa_to_county <- cbsa_to_county %>% 
  rename(CBSA = CBSAFP) %>% 
  mutate(FIPS = as.numeric(paste(STATEFP, formatC(COUNTYFP, width=3, flag="0"), sep="")))

# CHECK: There should be roughly 3000 FIPS codes and 900 CBSA codes
length(unique(cbsa_to_county$FIPS))
length(unique(cbsa_to_county$CBSA))


# weight employment statistics by teleworkability scores to estimate total number of teleworkable jobs
emp_data <- emp_data %>% 
  mutate(AREA = as.numeric(AREA)) %>% 
  rename(CBSA = AREA) %>% 
  left_join(tele_data, by = c("OCC_CODE", "OCC_TITLE")) %>% 
  select(CBSA, AREA_NAME, OCC_TITLE, TOT_EMP, teleworkable) %>% 
  drop_na(teleworkable) %>% 
  mutate(TOT_EMP = na_zero(as.numeric(TOT_EMP)), TOT_EMP_TEL = TOT_EMP * teleworkable)

# reshape data--x per industry w/ CBSA code observations, employment statistics as variables
emp_reshape <- function(df, x) {
  return(df %>% 
           group_by(CBSA, OCC_TITLE) %>% 
           summarize(.data[[x]]) %>% 
           dcast(CBSA ~ OCC_TITLE, fun.aggregate = NULL))
}
# employment data by cbsa and county, including proportion of teleworkable jobs
emp_cbsa <- list(emp_reshape(emp_data, "TOT_EMP"), emp_reshape(emp_data, "TOT_EMP_TEL"))
emp_cbsa <- emp_cbsa %>% 
  lapply(na_zero) %>% 
  lapply(function(x) cbind(x, rowSums(x[, 2:757])) %>% 
                     rename(TOT = ncol(x) + 1) %>% 
                     select(CBSA, TOT)) 
  
emp_cbsa <- left_join(emp_cbsa[[1]], emp_cbsa[[2]], by = "CBSA") %>% 
  rename(TOT_EMP = 2, TOT_TEL = 3)  %>% 
  mutate(pct_teleworkable = TOT_TEL / TOT_EMP)

emp_county <- cbsa_to_county %>% 
  select(CBSA, CBSA_TITLE, FIPS, NAMELSAD) %>% 
  left_join(emp_cbsa, by = c("CBSA")) %>% 
  drop_na(TOT_EMP)

# Check: See if it matches Dingel and Neiman paper (https://www.nber.org/system/files/working_papers/w26948/w26948.pdf) 
# when sorted by pct_teleworkable, should be ranked:
# 1 CBSA = 41940 (pct_teleworkable ~ .51), 2 47900 (~ .50), ...
# 100 CBSA = 15980 (pct_teleworkable ~ .28) 99 44700 (~ .28), ...
top_100 <- emp_cbsa[emp_cbsa[, "TOT_EMP"] >= 
                      sort(emp_cbsa$TOT_EMP, TRUE)[100], ]
head(arrange(top_100, desc(pct_teleworkable)), n = 10)
tail(arrange(top_100, desc(pct_teleworkable)), n = 10)
```
**Create Final Datasets**
```{r}
wks_snc_date <- function(vec, date) {
  return(as.numeric(as.Date(as.character(vec), format="%Y-%m-%d") - 
                      as.Date(as.character(date), format="%Y-%m-%d"))/7)
}

wks_snc_covid_indicators <- function(x) {
  for(i in seq(1, min(25, max(x$wks_snc_covid)))) {
    new_col <- as.integer(as.logical(x$wks_snc_covid == i))
    x <- cbind(x, new_col)
    colnames(x)[ncol(x)] <- paste("af", i, sep="")
  }
  
  for(i in seq(1, 25)) {
    new_col <- as.integer(as.logical(x$wks_snc_covid == -1*i))
    x <- cbind(x, new_col)
    colnames(x)[ncol(x)] <- paste("bf", i, sep="")
  }
  return(x)
}

final_data <- list(zearn_county, gtrends_county)

final_data <- final_data %>% 
  lapply(function(x) mutate(x, covid_date = as.Date(as.character("3/1/2020"), format="%m/%d/%Y"),
                            yr_start_date = as.Date(as.character("12/29/2019"),format="%m/%d/%Y"))) %>% 
  lapply(function(x) cbind(x, wks_snc_date(x$week, x$covid_date))) %>% 
  lapply(function(x) rename(x, wks_snc_covid = ncol(x))) %>% 
  lapply(function(x) cbind(x, wks_snc_date(x$week, x$yr_start_date) %% 52 + 1)) %>% 
  lapply(function(x) rename(x, wk_of_yr = ncol(x))) %>% 
  lapply(function(x) relocate(x, wk_of_yr, wks_snc_covid, .after=week))
# combine data
final_data <- final_data[[1]] %>% 
  full_join(final_data[[2]], by = c("FIPS", "wks_snc_covid", "wk_of_yr", "week", "covid_date", "yr_start_date"))
final_data <- final_data %>% 
  wks_snc_covid_indicators %>% 
  left_join(emp_county, by = c("FIPS")) %>% 
  left_join(hh_inc_county, by = c("FIPS")) %>% 
  select(-GEO_ID) %>% 
  left_join(pop_county, by = c("FIPS")) %>% 
  select(-GEO_ID, -NAME)
```
**Export Data**
```{r}
write_xlsx(final_data, path = "final_data.xlsx")
```


